{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 8490842,
     "sourceType": "datasetVersion",
     "datasetId": 5065701
    },
    {
     "sourceId": 8498424,
     "sourceType": "datasetVersion",
     "datasetId": 5071288
    },
    {
     "sourceId": 8509469,
     "sourceType": "datasetVersion",
     "datasetId": 5079545
    },
    {
     "sourceId": 8509507,
     "sourceType": "datasetVersion",
     "datasetId": 5079571
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "!pip install ultralytics"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-05-26T18:08:18.086772Z",
     "iopub.execute_input": "2024-05-26T18:08:18.087609Z",
     "iopub.status.idle": "2024-05-26T18:08:33.578944Z",
     "shell.execute_reply.started": "2024-05-26T18:08:18.087569Z",
     "shell.execute_reply": "2024-05-26T18:08:33.578016Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T18:04:52.363754Z",
     "start_time": "2024-05-27T18:04:51.171761Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (8.2.22)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (3.9.0)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (4.9.0.80)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (10.3.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (6.0.1)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (2.32.2)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (2.3.0)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (0.18.0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (4.66.4)\r\n",
      "Requirement already satisfied: psutil in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (5.9.8)\r\n",
      "Requirement already satisfied: py-cpuinfo in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (0.1.1.post2209072238)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (2.2.2)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from ultralytics) (0.13.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (4.52.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (6.4.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\r\n",
      "Requirement already satisfied: filelock in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.14.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (4.12.0)\r\n",
      "Requirement already satisfied: sympy in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2024.5.0)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.19.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alialghamdi/miniconda3/envs/deep_learning/lib/python3.9/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U ipywidgets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T18:08:33.580915Z",
     "iopub.execute_input": "2024-05-26T18:08:33.581218Z",
     "iopub.status.idle": "2024-05-26T18:08:47.124906Z",
     "shell.execute_reply.started": "2024-05-26T18:08:33.581190Z",
     "shell.execute_reply": "2024-05-26T18:08:47.123653Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T18:04:52.801078Z",
     "start_time": "2024-05-27T18:04:52.365438Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from ultralytics import YOLO",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T18:08:47.126286Z",
     "iopub.execute_input": "2024-05-26T18:08:47.126660Z",
     "iopub.status.idle": "2024-05-26T18:08:51.541486Z",
     "shell.execute_reply.started": "2024-05-26T18:08:47.126630Z",
     "shell.execute_reply": "2024-05-26T18:08:51.540486Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:16:11.886492Z",
     "start_time": "2024-05-28T17:16:07.670619Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "#### Training the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "results = model.train(\n",
    "    data='/kaggle/input/80-10-10-dataset/data.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    patience=50,\n",
    "    device=0,\n",
    "    batch=64,\n",
    "    optimizer='SGD',\n",
    "    momentum=0.95\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T18:04:56.645228Z",
     "start_time": "2024-05-27T18:04:56.645171Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = YOLO(\"best_80_10_10.pt\")\n",
    "\n",
    "metrics = model.val()\n",
    "metrics.box.map\n",
    "metrics.box.map50\n",
    "metrics.box.map75\n",
    "metrics.box.maps"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### hyperparameter-tuning\n",
    "\n",
    "- in our case SGD as optimizer is better that AdamW\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = YOLO(\"best_s_version.pt\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T18:13:53.945523Z",
     "iopub.execute_input": "2024-05-26T18:13:53.945933Z",
     "iopub.status.idle": "2024-05-26T18:13:53.995167Z",
     "shell.execute_reply.started": "2024-05-26T18:13:53.945903Z",
     "shell.execute_reply": "2024-05-26T18:13:53.994307Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- lr0 = [0.0005, **0.001**, 0.005]\n",
    "- lrf = [0.01, **0.005**]\n",
    "- batch = [16, **32**, 64]\n",
    "- momentum = [0.85, **0.9**, 0.95]\n",
    "- warmup_momentum = [**0.5**]\n",
    "- warmup_bias_lr = [**0.05**, 0.1, 0.2]\n",
    "- weight_decay = [0.0001, **0.0005**, 0.001]\n",
    "- patience = [**30**, 50]\n",
    "- imgsz = [**640**, 800, 1024]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data='/kaggle/input/80-10-10-dataset/data.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    patience=30,\n",
    "    device=0,\n",
    "    batch=32,\n",
    "    optimizer='SGD',\n",
    "    lr0=0.001,\n",
    "    lrf=0.005,\n",
    "    momentum=0.9,\n",
    "    warmup_momentum=0.5,\n",
    "    warmup_bias_lr=0.05,\n",
    "    weight_decay=0.0005,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "import cv2"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T17:31:07.855387Z",
     "start_time": "2024-05-28T17:31:02.916319Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "model = YOLO(\"best_s_version.pt\")\n",
    "video_path = 'sample_02.mp4'\n",
    "cap = cv2.VideoCapture(video_path)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T23:24:09.423656Z",
     "start_time": "2024-05-27T23:24:08.270833Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_s_version.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mYOLO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_s_version.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \n\u001B[0;32m      2\u001B[0m video_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample_02.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      3\u001B[0m cap \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mVideoCapture(video_path)\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001B[0m, in \u001B[0;36mYOLO.__init__\u001B[1;34m(self, model, task, verbose)\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m \u001B[38;5;241m=\u001B[39m new_instance\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# Continue with default YOLO initialization\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\ultralytics\\engine\\model.py:152\u001B[0m, in \u001B[0;36mModel.__init__\u001B[1;34m(self, model, task, verbose)\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(model, task\u001B[38;5;241m=\u001B[39mtask, verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 152\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\ultralytics\\engine\\model.py:241\u001B[0m, in \u001B[0;36mModel._load\u001B[1;34m(self, weights, task)\u001B[0m\n\u001B[0;32m    238\u001B[0m weights \u001B[38;5;241m=\u001B[39m checks\u001B[38;5;241m.\u001B[39mcheck_model_file_from_stem(weights)  \u001B[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Path(weights)\u001B[38;5;241m.\u001B[39msuffix \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 241\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mckpt \u001B[38;5;241m=\u001B[39m \u001B[43mattempt_load_one_weight\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moverrides \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset_ckpt_args(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\ultralytics\\nn\\tasks.py:806\u001B[0m, in \u001B[0;36mattempt_load_one_weight\u001B[1;34m(weight, device, inplace, fuse)\u001B[0m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattempt_load_one_weight\u001B[39m(weight, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, fuse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    805\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 806\u001B[0m     ckpt, weight \u001B[38;5;241m=\u001B[39m \u001B[43mtorch_safe_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# load ckpt\u001B[39;00m\n\u001B[0;32m    807\u001B[0m     args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mDEFAULT_CFG_DICT, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_args\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))}  \u001B[38;5;66;03m# combine model and default args, preferring model args\u001B[39;00m\n\u001B[0;32m    808\u001B[0m     model \u001B[38;5;241m=\u001B[39m (ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mema\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m ckpt[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()  \u001B[38;5;66;03m# FP32 model\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\ultralytics\\nn\\tasks.py:732\u001B[0m, in \u001B[0;36mtorch_safe_load\u001B[1;34m(weight)\u001B[0m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m temporary_modules(\n\u001B[0;32m    726\u001B[0m         {\n\u001B[0;32m    727\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124multralytics.yolo.utils\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124multralytics.utils\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    730\u001B[0m         }\n\u001B[0;32m    731\u001B[0m     ):  \u001B[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001B[39;00m\n\u001B[1;32m--> 732\u001B[0m         ckpt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# e.name is missing module name\u001B[39;00m\n\u001B[0;32m    735\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\serialization.py:997\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    995\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 997\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    998\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    999\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\serialization.py:444\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    443\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 444\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\serialization.py:425\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 425\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'best_s_version.pt'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "\n",
    "    for r in results:\n",
    "        annotator = Annotator(frame)\n",
    "\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            b = box.xyxy[0]\n",
    "            c = box.cls\n",
    "            annotator.box_label(b, model.names[int(c)])\n",
    "\n",
    "    frame = annotator.result()\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T17:17:23.100539Z",
     "start_time": "2024-05-28T17:17:23.094723Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alarm Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T16:43:04.562643Z",
     "start_time": "2024-05-28T16:42:56.711113Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pydub pyaudio",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\r\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting pyaudio\r\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hUsing cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\n",
      "Building wheels for collected packages: pyaudio\r\n",
      "  Building wheel for pyaudio (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyaudio: filename=PyAudio-0.2.14-cp39-cp39-linux_x86_64.whl size=27654 sha256=d8c2d585db6ba3d18885751dbc67334b1d66c3928933202efb1d08a6d1062339\r\n",
      "  Stored in directory: /home/mo/.cache/pip/wheels/28/d3/62/6ad369dc09fe82e1c9ceb83601a800eb305b901df7789aa550\r\n",
      "Successfully built pyaudio\r\n",
      "Installing collected packages: pydub, pyaudio\r\n",
      "Successfully installed pyaudio-0.2.14 pydub-0.25.1\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from alarm import AlarmDetector\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:31:07.878587Z",
     "start_time": "2024-05-28T17:31:07.857793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo/miniconda3/envs/yolo/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "model = YOLO(\"../models/best_s_version.pt\")\n",
    "video_path = '../video_samples/sample_01.mp4'\n",
    "alarm_sound = AudioSegment.from_file('alarm.wav')\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "calc = AlarmDetector(model, 300, 50)\n",
    "alarm = False\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read(10)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame, stream=True)\n",
    "\n",
    "    for r in results:\n",
    "        time.sleep(.01)\n",
    "        annotator = Annotator(frame)\n",
    "\n",
    "        boxes = r.boxes\n",
    "        calc.add_boxes(boxes)\n",
    "        for box in boxes:\n",
    "            b = box.xyxy[0]\n",
    "            c = box.cls\n",
    "            annotator.box_label(b, model.names[int(c)])\n",
    "\n",
    "        if calc.check_alarm() and not alarm:\n",
    "            alarm = True\n",
    "            play(alarm_sound)\n",
    "            # add to database and send notifications\n",
    "\n",
    "    frame = annotator.result()\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:42:33.666137Z",
     "start_time": "2024-05-28T17:42:27.873019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 10.2ms\n",
      "Speed: 2.4ms preprocess, 10.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.8ms\n",
      "Speed: 3.0ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.0ms\n",
      "Drowning\n",
      "Speed: 2.4ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 1 Swimming, 12.2ms\n",
      "Drowning\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.2ms\n",
      "Drowning\n",
      "Speed: 3.3ms preprocess, 9.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 9.9ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 9.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 1 Swimming, 13.2ms\n",
      "Drowning\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 8.6ms\n",
      "Drowning\n",
      "Speed: 4.0ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 9.1ms\n",
      "Drowning\n",
      "Speed: 3.4ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.3ms\n",
      "Drowning\n",
      "Speed: 3.0ms preprocess, 9.3ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 11.3ms\n",
      "Drowning\n",
      "Speed: 3.2ms preprocess, 11.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.6ms\n",
      "Drowning\n",
      "Speed: 3.4ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 8.6ms\n",
      "Drowning\n",
      "Speed: 2.4ms preprocess, 8.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.0ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 9.8ms\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.6ms\n",
      "Drowning\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.2ms\n",
      "Drowning\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 10.1ms\n",
      "Drowning\n",
      "Speed: 2.4ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.8ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.0ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.1ms\n",
      "Drowning\n",
      "Speed: 2.5ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.5ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.2ms\n",
      "Drowning\n",
      "Speed: 2.8ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.0ms\n",
      "Drowning\n",
      "Speed: 2.8ms preprocess, 10.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.9ms\n",
      "Drowning\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.9ms\n",
      "Drowning\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.8ms\n",
      "Drowning\n",
      "Speed: 2.8ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.6ms\n",
      "Drowning\n",
      "Speed: 2.4ms preprocess, 9.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.3ms\n",
      "Drowning\n",
      "Speed: 3.0ms preprocess, 9.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.9ms\n",
      "Drowning\n",
      "Speed: 3.4ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.9ms\n",
      "Drowning\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.8ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.3ms\n",
      "Drowning\n",
      "Speed: 3.0ms preprocess, 9.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 9.4ms\n",
      "Drowning\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.2ms\n",
      "Drowning\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 1 Swimming, 12.0ms\n",
      "Drowning\n",
      "Speed: 3.2ms preprocess, 12.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.6ms\n",
      "Drowning\n",
      "Speed: 2.5ms preprocess, 9.6ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.1ms\n",
      "Drowning\n",
      "Speed: 2.8ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 9.4ms\n",
      "Drowning\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 8.1ms\n",
      "Drowning\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 7.5ms\n",
      "Drowning\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 8.3ms\n",
      "Drowning\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 7.4ms\n",
      "Drowning\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 7.7ms\n",
      "Drowning\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 8.9ms\n",
      "Drowning\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 8.9ms\n",
      "Drowning\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Drownings, 1 Swimming, 7.7ms\n",
      "Drowning\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 7.9ms\n",
      "Drowning\n",
      "Speed: 2.1ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 7.9ms\n",
      "Drowning\n",
      "Speed: 2.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 11.4ms\n",
      "Drowning\n",
      "Speed: 3.6ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 12.0ms\n",
      "Drowning\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 14.6ms\n",
      "Drowning\n",
      "Speed: 3.4ms preprocess, 14.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Swimming, 10.9ms\n",
      "Speed: 3.6ms preprocess, 10.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 10.9ms\n",
      "Speed: 3.2ms preprocess, 10.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.9ms\n",
      "Drowning\n",
      "Speed: 3.5ms preprocess, 10.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.8ms\n",
      "Drowning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 3.7ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.8ms\n",
      "Drowning\n",
      "Speed: 3.1ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.9ms\n",
      "Drowning\n",
      "Speed: 3.0ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 10.8ms\n",
      "Drowning\n",
      "Speed: 3.1ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 11.1ms\n",
      "Drowning\n",
      "Speed: 3.2ms preprocess, 11.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 11.7ms\n",
      "Drowning\n",
      "Speed: 3.2ms preprocess, 11.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 7.0ms\n",
      "Drowning\n",
      "Speed: 2.6ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 6.9ms\n",
      "Drowning\n",
      "Speed: 2.5ms preprocess, 6.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.8ms\n",
      "Speed: 3.7ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.0ms\n",
      "Speed: 3.7ms preprocess, 11.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.4ms\n",
      "Speed: 3.5ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Drowning, 11.0ms\n",
      "Drowning\n",
      "Speed: 3.4ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.2ms\n",
      "Speed: 3.1ms preprocess, 11.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.1ms\n",
      "Speed: 3.1ms preprocess, 11.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Swimming, 6.7ms\n",
      "Speed: 2.4ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Swimming, 6.6ms\n",
      "Speed: 2.7ms preprocess, 6.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Swimming, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Swimming, 10.9ms\n",
      "Speed: 3.1ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.1ms\n",
      "Speed: 3.1ms preprocess, 11.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 11.4ms\n",
      "Speed: 3.1ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:16:47.686422Z",
     "start_time": "2024-05-28T17:16:47.674116Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### activate the Tracker in Yolov8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ]
}

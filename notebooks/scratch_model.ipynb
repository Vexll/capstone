{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install roboflow",
   "id": "a98eafc2d526d6f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"lBm4iE4T4ZnkvuavmAWR\")\n",
    "project = rf.workspace(\"saharat-yyrhu\").project(\"swimmingxdrowning\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"tensorflow\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "target_height, target_width = 224, 224\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load image and resize\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [target_height, target_width])\n",
    "    # Normalize image to range [0, 1]\n",
    "    img = img / 255.0\n",
    "    return img"
   ],
   "id": "d70a644d9097ea16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_data(path, batch_size=20):\n",
    "    # Step 1: Parse the CSV file\n",
    "    annotations_df = pd.read_csv(path + '_annotations.csv')\n",
    "\n",
    "    # Step 2: Create a TensorFlow Dataset for images\n",
    "    image_paths = annotations_df['filename'].tolist()\n",
    "    image_paths = [path + p for p in image_paths]\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "\n",
    "    # Apply preprocessing to image dataset\n",
    "    image_dataset = image_dataset.map(load_and_preprocess_image)\n",
    "\n",
    "    bounding_boxes = annotations_df[['xmin', 'ymin', 'xmax', 'ymax']].astype('float32').values\n",
    "    class_labels = annotations_df['class'].apply(lambda x: 0 if x == 'Swimming' else 0).values\n",
    "\n",
    "    # Create datasets for bounding boxes and class labels\n",
    "    bounding_boxes_dataset = tf.data.Dataset.from_tensor_slices(bounding_boxes)\n",
    "    class_labels_dataset = tf.data.Dataset.from_tensor_slices(class_labels)\n",
    "\n",
    "    # Step 5: Combine image dataset with bounding boxes and class labels\n",
    "    temp_dict = {\n",
    "        'bounding_boxes': bounding_boxes_dataset,\n",
    "        'class_labels': class_labels_dataset,\n",
    "    }\n",
    "    data = tf.data.Dataset.zip((image_dataset, temp_dict))\n",
    "\n",
    "    data = (\n",
    "        data.shuffle(buffer_size=1000)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    return data"
   ],
   "id": "389cb9f608097d0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset = get_data('SwimmingXDrowning-4/train/')",
   "id": "4751d02ffcbc396d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset",
   "id": "333273c7732a45e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "CLASSES = 2\n",
    "input_size = 224\n",
    "\n",
    "\n",
    "def build_classifier(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D(2, 2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2, 2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2, 2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(CLASSES, activation='softmax')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_regressor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D(2, 2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2, 2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2, 2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(units='4')(inputs)(x)\n",
    "\n",
    "    return x"
   ],
   "id": "fba794784a485777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_feature_extractor(inputs):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_model_adaptor(inputs):\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_classifier_head(inputs):\n",
    "    return tf.keras.layers.Dense(CLASSES, activation='softmax', name = 'classifier_head')(inputs)\n",
    "\n",
    "def build_regressor_head(inputs):\n",
    "    return tf.keras.layers.Dense(units = '4', name = 'regressor_head')(inputs)\n",
    "\n",
    "def build_model(inputs):\n",
    "    \n",
    "    feature_extractor = build_feature_extractor(inputs)\n",
    "\n",
    "    model_adaptor = build_model_adaptor(feature_extractor)\n",
    "\n",
    "    classification_head = build_classifier_head(model_adaptor)\n",
    "\n",
    "    regressor_head = build_regressor_head(model_adaptor)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_head, regressor_head])\n",
    "\n",
    "    return model"
   ],
   "id": "714f56e542c53e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = build_model(tf.keras.layers.Input(shape=(input_size, input_size, 1,)))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "    loss = {'classifier_head' : 'categorical_crossentropy', 'regressor_head' : 'mse' }, \n",
    "    metrics = {'classifier_head' : 'accuracy', 'regressor_head' : 'mse' })\n",
    "    \n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(train_dataset, epochs=EPOCHS)"
   ],
   "id": "38534da26edaed3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6989065b63dc97f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
